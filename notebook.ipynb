{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22931b5",
   "metadata": {},
   "source": [
    "# sageBot Pipeline\n",
    "### This notebook is the a pipeline for team sageBot in particition of the Tunga AI Hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ac699",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d42c9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "llm_model = \"gpt-3.5-turbo-0301\" # llm model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763e803",
   "metadata": {},
   "source": [
    "### Document to perform Q&A \n",
    "### AWS Lambda Function Documentation is use for this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94766dd4",
   "metadata": {
    "height": 113
   },
   "outputs": [],
   "source": [
    "# Download document  over a url\n",
    "from urllib.request import urlretrieve\n",
    "url = (\"https://primarywater.blob.core.windows.net/tunga/lambda-dg-1-500.pdf\")\n",
    "filename = \"document.pdf\"\n",
    "\n",
    "urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345febc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# load document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "file = \"document.pdf\"\n",
    "docs = PyPDFLoader(file)\n",
    "\n",
    "docs = docs.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c5c94",
   "metadata": {
    "height": 130
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# create a text splitter instance\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs) # splits the docs into chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d37f4fb",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c35bf",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# path to database/vectorstores\n",
    "persist_directory = 'chroma/'\n",
    "\n",
    "!rm -rf ./chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8f595",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# generate embeddings for our document\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "\n",
    "vectordb.persist() # save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3d2fd",
   "metadata": {
    "height": 113
   },
   "outputs": [],
   "source": [
    "# Load document embeddings from a vectorstores db\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = 'chroma/'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33d4fe",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer try make up the up an answer as accurate as possible. If there is code enclose it in triple backtics. Always say \"I am an experiment, my answers may be inaccurate at the end.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807d1af",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,) # prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf4b75",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_model, temperature=0)  # loading llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance for memory\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd39a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a retrieval chain for Q&A\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e62779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "question = \"Title of the context given to you\"\n",
    "result = qa({\"question\": question})\n",
    "\n",
    "display(Markdown(result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52a972",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Question 1\n",
    "\n",
    "question = \"Give a brief summary of the context\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98039cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "question = \"Using the context, summarise lambda in simple terms please.\"\n",
    "result = qa({\"question\": question})\n",
    "\n",
    "display(Markdown(result[\"answer\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
